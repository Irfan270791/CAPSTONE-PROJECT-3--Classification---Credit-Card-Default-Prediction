{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irfan270791/CAPSTONE-PROJECT-3--Classification---Credit-Card-Default-Prediction/blob/main/Another_copy_of_IM_Credit_card_default_prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -   Credit Card Default Prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    Team\n",
        "##### **Team Member 1 - Irfan Momin\n",
        "##### **Team Member 2 - Sushil Ghodwinde\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's environment,credit card have become a lifeline for many individuals,thus banks issue credit cards to us.A credit card is a form of payment card that allows charges to be made againsta card line rather than the account holder's cash deposit.when someone uses a credit card to make a purchase,the account of that individual accumulates a balance that must be paid off each month.we now know that the most typical issue in given these types of offers is customers being unable to pay their bills, these people are what we call \"DEFAULTERS\". The credit card Default database is Taiwan's most comprehensive classified with unbalanced dataset, dataset of customer credit card default payments, which provides infromation on domestic around Taiwan from 6 months in 2005. There is wealth of information available for this purpose, including payment history and bill_amount, paid amount, nature of target, i.e. dafaulter, and so on. The primary goal of this project is to do Exploratory Data Analysis and to use Machine Learning algorithms aimed at forecasting the case of customers default payments in Taiwan accuracy of Defaulters on dataset.Prediction of credit card Defaulters our project is to look for patterns and explanations in the context and present the findings in dynamic and visual way.\n",
        "\n",
        "To perform Exploratory Data Analysis on CREDIT CARD DEFAULT PREDICTION,we import the python libraries such as Numpy,Pandas,Matplotlib,Seaborn and plot to display the analylysis dataset, as well as Sklearn preprocessing data using standard scaler and importing logistic regression,Random Forest Classifier, XG Boost Classifier for predicting Defaulter getting accuracy for finding CREDIT CARD DEFAULT PREDICTION and in Graphical form through bar plot the major characteristics of CREDIT CARD DEFAULT PREDICTION are summarised using statistical graphics and other data visualisation tools."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Irfan270791/CAPSTONE-PROJECT-3--Classification---Credit-Card-Default-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This initiative aims to forecast customer default payment in Taiwan. From the standpoint of risk management the predictive accuracy of the predicted chance of default will be more valuable than the binary outcome of classification - credible or not credible clients.we must determine which clients will fall behind on their credit card payments. Financial danger are demonstrating a trend regarding commercial bank credit risk as the financial industry has improved dramatically. As a result one of the most risk serious risk to commercial bank is the risk prediction of credit clients.The current project is being created in order to analyse and predict the above mentioned database.This research aims to identify credit card consumers who are more likely to default in the next month."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "%matplotlib inline\n",
        "\n",
        "# Libraries warning would assist in ignoring warning issued.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import necessary Statistical libraries.\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import chisquare\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# Import necessary libraries for feature engineerings.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import libraries for machine learning model.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Import libraries for hyperparameter tuning and metric score.\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, classification_report, RocCurveDisplay, ConfusionMatrixDisplay, confusion_matrix"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qaqa2JtuPLTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset.\n",
        "filepath = '/content/drive/MyDrive/credit card /default of credit card clients.xls'\n",
        "credit_df = pd.read_excel(filepath, header=1)"
      ],
      "metadata": {
        "id": "HgbOaM4qP3KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "credit_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "credit_df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of Rows is:{credit_df.shape[0]}, and The number of Columns are:{credit_df.shape[1]}\")"
      ],
      "metadata": {
        "id": "3-aPayKVRvLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "credit_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All Columns are Numerical.**"
      ],
      "metadata": {
        "id": "8XoxDECGSMMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_value = len(credit_df[credit_df.duplicated()])\n",
        "print(f\"The Number of duplicate values in the dataset are =\", duplicate_value)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "credit_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.bar(credit_df, color = 'red', sort= 'ascending', figsize=(12,3), fontsize= 12)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Taiwanese credit card company wants to better anticipate the chance of default for its customers and identify the primary elements that influence this likelihood. This would let the issuer decide who to offer a credit card and what credit limit to supply. It would also help the issuer gain the better understanding of their current and potential clients, which would drive their future strategy, including their plans to offer tailored credit products to their customers.\n",
        "\n",
        "\n",
        "The above Dataset has 30000 Rows and 25 Columns. There are no missing values and Duplicate values in the Dataset."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "credit_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "credit_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As the response variable in this study, a binary variable, defalut payment (YES = 1, NO = 2) was used.This study analysed the literature and used the following variables as explanatory variables:**\n",
        "* **ID** : Unique ID for each client.\n",
        "* **LIMIT_BAL** : Amount of the given credit(NT dollar): its includes both\n",
        "   the individual consumer credit and his/her family(Supplementary) credit.\n",
        "* **Gender** : Male =1 , Female = 2.\n",
        "* **Education** : Graduate school = 1, University = 2, High School=3,\n",
        "  others = 4.\n",
        "* **Marital Status ** : Married =1, Single =2, Others =3\n",
        "* **Age** : Age in Years.\n",
        "\n",
        "**History of Passed Payments.**\n",
        "\n",
        "From April to September of 2005, We tracked historical monthly payment records. The payback status is measured using the following scale:\n",
        "No Spending = -2, Paid in Full = -1, use of revolving credit(paid minimum only) = 0.\n",
        "\n",
        "Payment Delay for one month =1, Payment Delay for two month =2;..., Payment Delay for eight month =8, Payment Delay for Nine Months and above =9.\n",
        "\n",
        "* PAY_0 : Repayment status in September, 2005\n",
        "* PAY_2 : Repayment status in August, 2005\n",
        "* PAY_3 : Repayment status in July, 2005\n",
        "* PAY_4 : Repayment status in June, 2005\n",
        "* PAY_5 : Repayment status in May, 2005\n",
        "* PAY_6 : Repayment status in April, 2005\n",
        "\n",
        "**Amount of Bill Statment(NT Dollar).**\n",
        "\n",
        "* BILL_AMT1: Amount of bill statment in September, 2005\n",
        "* BILL_AMT2: Amount of bill statment in August, 2005\n",
        "* BILL_AMT3: Amount of bill statment in July, 2005\n",
        "* BILL_AMT4: Amount of bill statment in June, 2005\n",
        "* BILL_AMT5: Amount of bill statment in May, 2005\n",
        "* BILL_AMT6: Amount of bill statment in April, 2005\n",
        "\n",
        "\n",
        "**Amount of Previous Payment (NT Dollar)**\n",
        "\n",
        "* PAY_AMT1 : Amount of Previous Payment in September, 2005\n",
        "* PAY_AMT2:  Amount of Previous PAyment in August, 2005\n",
        "* PAY_AMT3:  Amount of Previous Payment in July, 2005\n",
        "* PAY_AMT4:  Amount of Previous Payment in June, 2005\n",
        "* PAY_AMT5:  Amount of Previous PAyment in May, 2005\n",
        "* PAY_AMT6:  Amount of Previous Payment in April, 2005\n",
        "* Defaul.Payment.Next.Month:  Default Payment(Yes=1, No=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in credit_df.columns.tolist():\n",
        "  print(\"Number of Unique values in\",i,\"is\",credit_df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#  Renaming the Dependent Variables.\n",
        "credit_df.rename(columns={'default payment next month' : 'IsDefaulter'}, inplace= True)\n",
        "\n",
        "# Changing the name of some columns for better understanding.\n",
        "credit_df.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'}, inplace= True)\n",
        "credit_df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace= True)\n",
        "credit_df.rename(columns={'PAY_AMT1': 'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'}, inplace= True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping ID columns because there is no use of ID further.\n",
        "credit_df.drop('ID', axis=1, inplace= True)"
      ],
      "metadata": {
        "id": "AMrA1l0YRTsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying the Orignal Dataset to diffrant Dataset for better visualisation.\n",
        "df_credit = credit_df.copy()"
      ],
      "metadata": {
        "id": "jreYSLBoRz5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the value with there labels.\n",
        "df_credit.replace({'SEX':{1: 'Male', 2:'Female'}}, inplace= True)\n",
        "df_credit.replace({'EDUCATION':{1: 'Graduate School', 2: 'University', 3: 'High School', 4:'Others', 5:'Others', 6:'Others', 0:'Others'}}, inplace= True)\n",
        "df_credit.replace({'MARRIAGE': {1: 'Married', 2: 'Single', 3: 'Others', 0: ' others'}}, inplace= True)\n",
        "df_credit.replace({'IsDefaulter': {1: 'Yes', 0: 'No'}}, inplace= True)"
      ],
      "metadata": {
        "id": "DRrMaw4FSSW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.head()"
      ],
      "metadata": {
        "id": "tjiowhu6WJxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We rename the Dependent Variables and Feature name to improve understanding of the Features.\n",
        "\n",
        "* Converting Numerical Values to Categorical values for easy comprehension\n",
        "* Gender : (1 = Male, 2 = Female)\n",
        "* Education : 1 = Graduate School, 2= University, 3= High School, 4= Others\n",
        "* Status of Marriage : (1= Married, 2= Single, 3= Others)\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1    Visualization of Defaulter VS Non-Defaulter."
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.pie(x=df_credit['IsDefaulter'].value_counts(),labels =['Non-Defaulter','Defaulter'],autopct='%1.1f%%')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie-Chart used to represent the summarizing set of nominal data or display the different value of a given categorical variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NO = Payment will not Default ,   YES = Payment will default.**\n",
        "\n",
        "From the above chart we can see that the of Defaulter(22%) is less than Non-Defaulter(78%)."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, The class imbalance can have negative impact on classification model accuracy, So we must address this issue before putting the data into training."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2   Visualization of Gender Column."
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig,ax=plt.subplots(1,2,figsize=(15,6))\n",
        "# Univariat analysis-distribution of male female ration.\n",
        "ax[0].pie(x=df_credit['SEX'].value_counts(), labels=['Female','Male'],autopct='%1.1f%%')\n",
        "ax[0].set_title('Distribution of Male and Female', fontsize =14, fontweight = 'bold')\n",
        "# Bivariate analysis - Defaulter and Non-Defaulter distribution between male and female\n",
        "sns.countplot(x= 'SEX', hue= 'IsDefaulter', data= df_credit,ax=ax[1])\n",
        "ax[1].set_title('Sex VS Defaulter', fontsize= 14, fontweight= 'bold')\n",
        "ax[1].set_xlabel('SEX', fontsize= 12)\n",
        "ax[1].set_ylabel('Count', fontsize= 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pie-Chart used to represent the summarizing set of Nominal data or\n",
        "  display the different value of a given categorical variable.  \n",
        "* The Count plot is used to display the number of occurrences of the observation in the categrical variable. For the visual representation, it employs the concept of a bar chart.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* The number of Female credit card holder is more than Male.\n",
        "* The number of Female Defaulter is more than Male Defaulter.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the Female credit card holders outnumber the Male credit card holders, thus in order to boost male client, bank should offer incentive to Male client while also taking care of their Female client in order to grow their business. However, some policies should be made availabel to Male clients in order to limit the possibility of default."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3    Visualization of Education Column."
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "fig,ax=plt.subplots(1,2,figsize=(15,6))\n",
        "#  Univariate analysis-distribution for Educational Qualification .\n",
        "ax[0].pie(x=df_credit['EDUCATION'].value_counts(),labels=df_credit['EDUCATION'].value_counts().index,autopct='%1.1f%%')\n",
        "ax[0].set_title('Distribution of Education Qualification of Customers', fontsize= 14, fontweight='bold')\n",
        "#  Bivariate analysis - Defaulter and Non-Defaulter distribution between male and Female\n",
        "sns.countplot(x = 'EDUCATION', hue= 'IsDefaulter', data= df_credit, ax=ax[1])\n",
        "ax[1].set_title('Education Qualification V/S Defaulter', fontsize=14, fontweight= 'bold')\n",
        "ax[1].set_xlabel('Education Qualification', fontsize= 12)\n",
        "ax[1].set_ylabel('Count', fontsize= 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Pie-Chart used to represent the summarizing set of nominal data or\n",
        "  display the different value of given categorical variable.\n",
        "* The Count plot is used to display the number of occurrences of the\n",
        "  observation in the categorical variable.For the visual representation, it employs the concept of bar chart.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Most of the customer's educational qualification is University pass out\n",
        "  followed by Graduate School pass out.\n",
        "* Most number of Defaulter is also from University but in terms of Default ratio university people have less Defaulte ratio, High School have most Default ratio.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see the majority of credit card holders are from Universities, followed by Graduate School, so bank can target this people to grow their business. As we know the source of income for High School candidate is very low and their Default ratio is also high, so there is no need to focus more on this group of people."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4    Visualization of Marriage Column."
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fix,ax=plt.subplots(1,2,figsize=(15,6))\n",
        "# Univariate analysis distribution for Marriage.\n",
        "ax[0].pie(x= df_credit['MARRIAGE'].value_counts(), labels= df_credit['MARRIAGE'].value_counts().index,autopct= '%1.1f%%')\n",
        "ax[0].set_title('Distribution of Marital Status of Customers', fontsize= 14, fontweight= 'bold')\n",
        "# Bivariate analysis - Defaulter and Non-Defaulter distribution between Male and Female.\n",
        "sns.countplot(x = 'MARRIAGE',  hue= 'IsDefaulter', data= df_credit, ax=ax[1])\n",
        "ax[1].set_title('Marital Status V/S Defaulters', fontsize= 14, fontweight= 'bold')\n",
        "ax[1].set_xlabel('Marital Status', fontsize= 12)\n",
        "ax[1].set_ylabel('Count', fontsize= 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pie-Chart used to represent the summarizing set of nominal data or display the different value of given categorical variable.\n",
        "* The Count plot is used to display the number of occurrences of the observation in the categorical variable.For the visual representation, it employs the concept of bar chart.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Maximum Card holders are Single.\n",
        "* Married people have higher default ratio.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single folks spend more than married people, thus they will used credit card more frequently. As a result, targeting single people will undoubtedly enhance sales."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5    Visualization of Age Columns.\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig,ax = plt.subplots(3,1,figsize=(15,15))\n",
        "# Univariate analysis - distribution for Marriage.\n",
        "age_dist = sns.histplot(x = df_credit['AGE'],ax=ax[0],bins=10)\n",
        "age_dist.set_title('Distribution of Age of Credit Card Holder', fontsize=14, fontweight='bold')\n",
        "# Creating Two Dataframe of Male & Female.\n",
        "Age_male = df_credit[df_credit['SEX']=='Male']\n",
        "Age_female = df_credit[df_credit['SEX']=='Female']\n",
        "sns.countplot(x = 'AGE',hue= 'IsDefaulter', data= Age_male,ax=ax[1])\n",
        "# Ploting the age of Male Card holder vs Defaulter.\n",
        "ax[1].set_title('Age of Male V/S Defaulter', fontsize= 14, fontweight='bold')\n",
        "ax[1].set_xlabel('Age', fontsize=12)\n",
        "ax[1].set_ylabel('Count', fontsize=12)\n",
        "# Ploting the age of Female card holder vs Defaulter.\n",
        "sns.countplot(x = 'AGE', hue= 'IsDefaulter', data= Age_female, ax=ax[2])\n",
        "ax[2].set_title('Age of Female V/S Defaulter', fontsize=14, fontweight='bold')\n",
        "ax[2].set_xlabel('Age', fontsize=12)\n",
        "ax[2].set_ylabel('Count', fontsize =12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* A histogram is a traditional visualization tool that count the number of data that fall into discrete bin to illustrate the distribution of one or more variables.\n",
        "* The Countplot is used to display the number of occurrences of the observation in the categorical variable. For visual representation, its employ the concept of bar chart.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most of the Credit Card holder belongs to 25-30 Years.\n",
        "* Number of Credit Card holder are less above 60 Years.\n",
        "* For Equal Age ratio of Defaulter for Male and Female not varied much.\n",
        "* Above 50 Year old the ratio of Defaulter has increased."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers are fewer as the majority of Defaulter are between the age of 20 and 25, and over age of 60. Before issuing the credit card, it must be roughly reviewed. Otherwised it may have a negative impact on corpor\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6    Visualization of Default Payment Next Month Column.\n"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig,ax = plt.subplots(3,1,figsize=(15,25))\n",
        "# Univariate analysis - distribution of Marriage.\n",
        "age_dist= sns.histplot(x= df_credit['LIMIT_BAL'],ax=ax[0],bins=10)\n",
        "age_dist.set_title('Distribution of Balance Limit of Credit Card Holder', fontsize= 14, fontweight= 'bold')\n",
        "Limit_1 = df_credit[df_credit['LIMIT_BAL']<= 500000]\n",
        "Limit_2 = df_credit[df_credit['LIMIT_BAL']> 500000]\n",
        "# Ploting the Age of male Credit Card Holder VS Defaulter.\n",
        "sns.countplot(x = 'LIMIT_BAL', hue= 'IsDefaulter', data= Limit_1, ax=ax[1])\n",
        "ax[1].set_title('Balance Limit VS Defaulter', fontsize= 14, fontweight= 'bold')\n",
        "ax[1].tick_params(axis = 'x', labelrotation= 60)\n",
        "ax[1].set_xlabel('Balance Limit', fontsize= 12)\n",
        "ax[1].set_ylabel('Count', fontsize= 12)\n",
        "# Ploting the age of male Credit card holer VS Defaulter.\n",
        "sns.countplot(x= 'LIMIT_BAL', hue= 'IsDefaulter', data= Limit_2, ax=ax[2])\n",
        "ax[2].set_title('Balance Limit VS Defaulter', fontsize=14, fontweight= 'bold')\n",
        "ax[2].tick_params(axis = 'x', labelrotation= 60)\n",
        "ax[2].set_xlabel('Balance Limit', fontsize= 12)\n",
        "ax[2].set_ylabel('Count', fontsize= 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* A histogram is a traditional visualization tool that count the number of data that fall into discrete bins to illustrate the distribution of one or more variables.\n",
        "* The Countplot is used to display the number of occurrences of the observation in the categorical variable. For the visual representation, its employ the concept of Bar chart.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Maximum Credit Card limit is below 100000.\n",
        "* Very few Credit limit are above 500000.\n",
        "* When Credit limit is less than 50k default ratio is high.\n",
        "* Default ratio is exceptionally high when credit limit is 550000 and 600000\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customers with credit limit of $50,000 or more likely to default, so the bank should charge them in the event of default and may cut their limit to avoid any form of damage to the company."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7    Monthly wise repayment VS Defaulter."
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Creating the list of monthly repayment columns.\n",
        "repay_col = ['PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']\n",
        "# Ploting the Univariate distribution.\n",
        "fig,ax = plt.subplots(len(repay_col),1,figsize=(10,30))\n",
        "for i,col in enumerate(repay_col):\n",
        "  # Ploting the bivariate with isdefaulter.\n",
        "  sns.countplot(x = col, hue= 'IsDefaulter', data= df_credit, ax=ax[i])\n",
        "  ax[i].set_title(col)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Countplot is used to display the number of occurrences of the observation in the categorical variable, For the visual representation it employes the concept of bar chart."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the customer who pays only the minimum amount have a higher risk of default. customers in this category also mostly use credit card.\n",
        "\n",
        "When payment delya is greater than 2 months default ratio is very high."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Payment delay is greater than 2 month, default ratio is high , so it has negative impact on buisness. So when customer is delying 1 month company should concerned."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8    Visualization of Monthly Billing Amount.\n",
        "\n"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "monthlybill_col = ['BILL_AMT_SEPT','BILL_AMT_AUG','BILL_AMT_JUL','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR']\n",
        "# Ploting the univariate distribution.\n",
        "fig,ax = plt.subplots(len(monthlybill_col),1,figsize=(10,30))\n",
        "for i,col in enumerate(monthlybill_col):\n",
        "  # ploting the bivariate with isdefaulter.\n",
        "  sns.boxplot(x = col, y = 'IsDefaulter', data= df_credit, ax=ax[i])\n",
        "  ax[i].set_title(col)"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplot provide a rapid visual assessment of a dataset's variability. They display the dataset's Median, upper and lower quartiles , lowest and maximum values and any outliers. Outlier in data might show errors or uncommon events."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plot we can see that the data distribution of monthly bill has no thick difference between defaulter and non defaulter."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no insights on business from the figures."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9   Visualization of Amount of Previous Payment VS Bill Amount"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "previous_col = ['PAY_AMT_SEPT','PAY_AMT_AUG','PAY_AMT_JUL','PAY_AMT_JUN','PAY_AMT_MAY','PAY_AMT_APR']\n",
        "# Ploting the univariate distribution.\n",
        "fig,ax = plt.subplots(len(previous_col),1,figsize=(10,30))\n",
        "for i,col in enumerate(previous_col):\n",
        "  # Ploting the bivariate with isdefaulter.\n",
        "  sns.scatterplot(x = col, y = monthlybill_col[i], hue='IsDefaulter', data= df_credit, ax=ax[i])\n",
        "  ax[i].set_title(col)"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Scatterplot is used to plot how much a numerical feature is a affected by another numerical value."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Previous Payment is low but Monthly Bill is high, there is high possibility of default the payment."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Customer is spending exceptionally high according to his payment history, company should concerned."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10   Correlation Heatmap"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure (figsize=(20,10))\n",
        "sns.heatmap(df_credit.corr(), linewidths=.5, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmap are graphical representation of the strength of corrrelation between numerical data. Correlation plot are used to determind which variable are related to one another and how strong this relationship is. A correlation plot often has several numerical variable, each represents the by a columns. Each row represents the relationship between two variables.\n",
        "\n",
        "favourable numbers imply a favourable relationship, wheras negative values suggest a negative relationship."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the heatmap, some features(for example PAY_MAY & PAY_APR, BILL_AMT_MAY & BILL_AMT_APR,etc) are highly associated to each other, but we are not going to eliminate any of them because they contain clint transaction details."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11  Pair Plot"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "plt.figure(figsize=(20,40))\n",
        "sns.pairplot(df_credit,vars=['BILL_AMT_SEPT','BILL_AMT_AUG','BILL_AMT_JUL','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR','PAY_AMT_SEPT','PAY_AMT_AUG','PAY_AMT_JUL','PAY_AMT_JUN','PAY_AMT_MAY','PAY_AMT_APR'], kind='scatter',hue='IsDefaulter')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Seaborn pairplot depicts pairwise relationships between variables within a dataset. This offers good visualization and helps us graps the material by condensing a big quantity of data into a single image. This is critical as we explore and become acquainted with our dataset."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the prior month's bill amount is linearly connected, implying that user spend a comparable amount each month. Higher billing amount from prior months may be defaulters in the future. Where is payment amount is different each month."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 -"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 14 visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 -"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "\n",
        "\n",
        "**Married Male Defaults average age is 35 Year.**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis :N = 35\n",
        "Alternate Hypothesis: N!= 35\n",
        "\n",
        "Test type : Two Tailed Test."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "sample_1 = df_credit[(df_credit['MARRIAGE']=='Married') & (df_credit['SEX']=='Male') & (df_credit['IsDefaulter']=='Yes')].sample(1000)\n",
        "# Getting the required parameter value for Hypothesis testing.\n",
        "N = 35\n",
        "sample_mean = sample_1['AGE'].mean()\n",
        "size = len(sample_1)\n",
        "# The Standard deviation for population\n",
        "std_pop = df_credit['AGE'].std()\n",
        "Z_stat = ((sample_mean - N)/ (std_pop/np.sqrt(N)))\n",
        "\n",
        "# Calculate the z value.\n",
        "z_value = norm.cdf(Z_stat,0,1)\n",
        "# Calculte the p value.\n",
        "if z_value>0.5:\n",
        "  p_value = 2*(1-z_value)\n",
        "else :\n",
        "    p_value= 2*z_value\n",
        "print(f'P value is {p_value}')\n",
        "if p_value>=0.05:\n",
        "  print('Fail to Rejcet Null Hypothesis')\n",
        "else:\n",
        "    print('Rejaect the Null Hypothesis')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilised the Z-Test as a statistical test to produce P-value and discovered that the Null Hypothesis was Rejected and that married male customer defaulting do not have an average age of 35 yea"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution Plot for Age Feature.\n",
        "fig,ax = plt.subplots(1,1,figsize=(9,6))\n",
        "feature = (sample_1[\"AGE\"])\n",
        "sns.distplot(sample_1[\"AGE\"])\n",
        "ax.axvline(feature.mean(),color='magenta',linestyle = 'dashed', linewidth = 2)\n",
        "ax.axvline(feature.median(),color = 'cyan', linestyle = 'dashed', linewidth = 2)\n",
        "ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_dAz22l6gTou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference = sample_1[\"AGE\"].mean()-sample_1[\"AGE\"].median()\n",
        "print(\"Mean Median difference is :\" , mean_median_difference)"
      ],
      "metadata": {
        "id": "GrtWAbDkizhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As illustrated above, the mean median differance is nearly zero. The Mean is roughly the same as the median.As a result it is a Normal Distribution, that why we used Z-Test directly."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "\n",
        "\n",
        "**Customers defaulting have an average credit limit of 90000.**"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: mean = 90000\n",
        "\n",
        "Alternate Hypothesis: mean != 90000\n",
        "\n",
        "Type of Test = Two Tailed Test."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "sample_2 = df_credit[(df_credit['IsDefaulter']=='Yes')].sample(1000)\n",
        "# Getting the required parameter values for hypothesis testing.\n",
        "N = 90000\n",
        "sample_mean = sample_2['LIMIT_BAL'].mean()\n",
        "size = len(sample_2)\n",
        "# Calculating the t_statistic and p_value.\n",
        "t_statistic,p_value = stats.ttest_1samp(sample_2['LIMIT_BAL'],N)\n",
        "print('T-Statistic value', t_statistic)\n",
        "print(\"P-value\", p_value)\n",
        "if p_value >= 0.05:\n",
        "  print('Fail to Reject the Null Hypothesis')\n",
        "else:\n",
        "    print('Reject the Null Hypothesis')\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilised the T-test as a Statistical test to generate P-value and discovered that the null hypothesis was rejected since the customer that defaulted had not an average credit limit of 90000."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_median_difference = sample_2['LIMIT_BAL'].mean()- sample_2['LIMIT_BAL'].median()\n",
        "print(\"Mean Median Difference is :\", mean_median_difference)"
      ],
      "metadata": {
        "id": "5DDEMtKjpI9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Accompanying figure shows that the median is bigger than the mean. As result, the distribution is positively biassed. Z-Test can not be conducted on skewed data.\n",
        "\n",
        "Even with strongly Skewed data, t-test and their related confidance interval can and should be utilised in investigation with a large sample size.\n",
        "\n",
        "So,with the  skewed data, we can utilised the T-Test to get a better result, That why we used the t-test."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis(HO): Defaulter does not depends on Educational Qualification.\n",
        "\n",
        "Alternate Hypothesis(Ha): Defaulter also get affected by Educational Qualification."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Creating contigency table of Education and Default Payment Next Month.\n",
        "cont_table = pd.crosstab(df_credit['IsDefaulter'], df_credit['EDUCATION'], margins= False)\n",
        "# Using Chi-Squre test to validate Null / Alternate Hypothesis.\n",
        "stat, P_value, dof, expected = chi2_contingency(cont_table)\n",
        "print(\"P-value\", P_value)\n",
        "if P_value >= 0.05:\n",
        "  print('Fail to Reject Null Hypothesis')\n",
        "else :\n",
        "     print('Reject the Null Hypothesis')"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Chi-Squred test to genrate the P_Value and discovered Null hypothesis was rejected, Since defaulter does not depend on educational qualification"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have to test difference between defaulter and non defaulter with educational qualification. The chi-squre test is a statistical procedure for determining the difference between observed and expected data."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the orignial data before applying feature engineering.\n",
        "df = df_credit.copy()"
      ],
      "metadata": {
        "id": "1AQGzflGv4TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no missing value in dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Checking Outliers in dataset using boxplot.\n",
        "for col in df.describe().columns:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.boxplot(x=df[col])"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the boxplot to look for outliers in the data set. Some points appear to be outliers in numerical features.that are primarily connected to quantity, and we used the IQR method to treat these outliers without knowing the exact limit of these columns from the bank, and addressing these outlying amount will result in data loss and there is potential that the amount that was maximal and appeared to be an outlier was the true value for that characteristic, thus we did not employ any approaches to treat these outlying numbers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the dependable columns categorical to binary.\n",
        "df.replace({'IsDefaulter': {'Yes': 1, 'No': 0}}, inplace= True)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking How many columns contain categorical data.\n",
        "sum(df.dtypes==object)"
      ],
      "metadata": {
        "id": "nY9QPuoTv8TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dummies.\n",
        "df = pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "omCEqISHwT9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "nO9g7yd6wkME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-shot encoding is critical step in preparing your dataset for machine learning. Your categorical data is converted to a binary vector representation via One-shot encoding. Panadas get dummies make this really simple. This is critical for working with many machine learning algorithem that accept only numerical inputs, such as decision trees and support vector machines. This indicates that a new column is formed for each unique value in a columns. Depending on whether the value matches the columns header, the value in the columns represented as 1s or 0s"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no need for feature manipulation or feature selection because the only previous months payment status and previous months bill amount have a high correlation and cannot be dropped or manipulated because they are the most significant parameters of detect defaulter."
      ],
      "metadata": {
        "id": "bWEU2V8l0ZqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a function to check the multicolinearity.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "  # Calculating VIF\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"variables\"] = X.columns\n",
        "  vif[\"VIF\"] = [round(variance_inflation_factor(X.values, i),2) for i in range(X.shape[1])]\n",
        "\n",
        "  return(vif)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Multicolinearity.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['IsDefaulter','MARRIAGE_others','EDUCATION_High School','SEX_Male','SEX_Female','EDUCATION_University','EDUCATION_others','MARRIAGE_married','MARRIAGE_Single','EDUCATION_Graduate School']]]).sort_values(by='VIF', ascending= False)"
      ],
      "metadata": {
        "id": "01PfBpQb38Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping Sex_female columns.\n",
        "df.drop(['SEX_Female'],axis=1,inplace= True)"
      ],
      "metadata": {
        "id": "-r5TTPnn6yxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Variance Inflation Factor(VIF) is used to detect multicolinearity. Variance Inflation Factor(VIF) quantify how much the variance of predicted regression coefficients is inflated when the predictor variables are not linearly connected."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the VIF value of a few columns is larger than 10, we select all features since those columns are most critical feature and we cannot afford to loss them. Only SEX_Female column we have dropped because it is unnecessary of keeping two same binary column."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of all independent features.\n",
        "for col in df.describe().columns:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature= (df[col])\n",
        "  sns.distplot(df[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle = 'dashed', linewidth= 2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle = 'dashed', linewidth = 2)\n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8BWsOZVk916t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no need to transformed any features."
      ],
      "metadata": {
        "id": "Slq-7XK9ADOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "41C48Q_fDGT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No There is no need for Dimesionality Reduction in our dataset."
      ],
      "metadata": {
        "id": "tjdQ27a5Dame"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "_yJ3vJfLDtF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "KyjhWzMBD0Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "MGNyosJfD5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "eQIQk5ZjEA4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "75blt5bBEElq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "-5iZH_9OEV84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting dependent and indipendent variable.\n",
        "y= df['IsDefaulter']\n",
        "X= df.drop(['IsDefaulter'], axis=1)"
      ],
      "metadata": {
        "id": "cPaYjFIxFN9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "Tho98UbrEkZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of class imbalance.\n",
        "y.value_counts().plot(kind='pie',\n",
        "                      figsize=(15,6),\n",
        "                      autopct = \"%1.1f%%\",\n",
        "                      startangle = 90,\n",
        "                      shadow= True,\n",
        "                      labels = ['Not Defaulters(%)', 'Defaulters(%)'],\n",
        "                      colors = ['green','red'],\n",
        "                      explode = [.05, .05]\n",
        "                      )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bWYThnYtF5DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, 77.9% of data belongs to Not Defaulters category, while only 22.1% belongs to Defaulters, implying that our dataset is unbalanced.\n",
        "\n",
        "In a dataset with highly imbalanced classes, the classifier will always \"Predict\" the most common class without performing any feature analysis and will have high accuracy rate, despite the fact that it is plainly not the correct one.\n",
        "\n",
        "The balanced data would imply that each class would receive 50% of the available points. Little imbalance is not concern for the most machine learning techniques. So if One class has 60% of the points and other has 40%, there should be no noticeable performance reduction. Only when the class imbalance is extrem, such as 90% for One Class and 10% for the other, would typical optimisation criteria or performance metrics be ineffective and require adjustment."
      ],
      "metadata": {
        "id": "Ze763KS8Erko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Importing SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "print('Dataset Before: \\n', y.value_counts())\n",
        "smt = SMOTE(random_state=40)\n",
        "\n",
        "# Fitting the predictor and target variable.\n",
        "X_smt, y_smt = smt.fit_resample(X,y)\n",
        "\n",
        "print('\\nDataset After:\\n', y_smt.value_counts())"
      ],
      "metadata": {
        "id": "6AFqUrUmExxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "gnQ1QshME6YV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE (Synthetic Minority Over-sampling technique) was employed to balance the dataset.\n",
        "\n",
        "SMOTE is machine learning technique for coping with problem that arise while working with an imbalanced data collection. In practise imbalanced data set are common, and most ML algorithm are valunerable to them, so we must improve their performance by employing approaches such as SMOTE.\n",
        "\n",
        "The SMOTE algorithm operates in Four simple steps.\n",
        "\n",
        "1. Select a minority group as the inpute vector.\n",
        "2. Determine its K Nearest Neighbour(k_neighbour is an inpute to the SMOTE\n",
        "   () method)\n",
        "3. Select one of these neighbour and draw a synthetic point somewhere on the line connecting the point under consideration and its selected neighbour.\n",
        "4. Repeat the process until the data is balanced.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9tx_oienFDly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "OROCp1U9Qq-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_smt, y_smt, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "8Vf8ucKAQ0yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of tain dataset.\n",
        "print(X_train.shape, y_train.shape)\n",
        "# Checking  the shape of test dataset.\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "WmCSVepARrVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "IAjpWTV9Q5o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used an 80:20 split for the train-test split. We can see that we have 37382 data for training and 9346 data for test. Which is reasonable split to being with because we have to kept a substantial amount of data for training our model."
      ],
      "metadata": {
        "id": "eVsvfZJBQ_lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose Standard Scaler to scale our data since the data is adjusts so mean is 0 and Standard deviation is 1. In a nutshell, it Standardises the data. Standardisation is beneficial for data with negative value. The data is arranged in a conventional normal distribution. its better for classification"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1   Logistic Regression."
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "model_lr = LogisticRegression(fit_intercept=True, max_iter=1000)\n",
        "# Fit the Algorithm\n",
        "model_lr.fit(X_train_scaled,y_train)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Model coefficient.\n",
        "print(\"The Coefficient obtain from logisticregression model \", model_lr.coef_)\n",
        "# Checking the intercept value.\n",
        "print(\"The Coefficient obtain from logisticregression model\", model_lr.intercept_)"
      ],
      "metadata": {
        "id": "q9r09KnYDTnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the probabilities of  test data.\n",
        "lr_train_proba = model_lr.predict_proba(X_train_scaled)\n",
        "lr_test_proba = model_lr.predict_proba(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "HlU9vGI4EV2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the value of y from x via model.\n",
        "y_pred_test = model_lr.predict(X_test_scaled)\n",
        "y_pred_train = model_lr.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "LqByHYEMFdWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a classification algorithm that uses supervised learning to predict the likehood of target variable. Because the nature of the target or dependent variable is dichotomous, there are only two variable classes.\n",
        "\n",
        "Simply put,the dependent variable is binary in nature, with data represented as either 1(for success/Yes) or 0(for failure/No).\n",
        "\n",
        "A logistic regression model predict P(Y=1) as a funciton of X mathematically. it is one of the most basic ML technique that may be used to solve a varity of classification issue such as spam identification, diabetes prediction , cancer diagnosis, and so on."
      ],
      "metadata": {
        "id": "0p5yg_mNGEzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation metrics."
      ],
      "metadata": {
        "id": "DXG5JiNdIOB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy **  Accuracy=number of correct forecast.Total number of forecast.\n",
        "\n",
        "**Precision** Precision is defined as the number of genuine positives divided by the number of anticipated positives given label. When the cost of false positives (FP) is high, precision is good statistic to utilise.\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "**Recall** Recall is defined as the number of true positive divided by the total number of actual positive for a label.When the cost of false negative(FN) is high, recall is excellent statistic to employ.\n",
        "\n",
        "recall = TP / (TP + FN)\n",
        "\n",
        "**F1-Score** The harmonic mean of precision and recall is defined as the F-1 Score.\n",
        "\n",
        "**AUC-ROC**: The Receiver Operator Characteristic (ROC) cureve is a binary classification issue evaluation metric. it is probability curve that plot the TPR aginst FPR at various threshold levels and essentially separates the 'signal' from the 'noise'. The area under the curve (AUC) is a measure of a classifiers ability to distinguish between classes and it used to summarise the ROC curve.\n",
        "\n",
        "We will be using Recall primarily for model evalution because False negative indicates that person will not default when they actually do and recognising defaulters clint as non defaulter will result in a large loss for the bank, thus we must reduce False Negative and as False Negative Decreases , Recall will grow.\n",
        "\n",
        "**KS-plot** Kolmogorov-Smirnov chart measure preformance of classification model.K-S is measure of the degree of separation between the positive and negative distributions."
      ],
      "metadata": {
        "id": "bkNKGDT2IZLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install scikit-plot library\n",
        "! pip install scikit-plot"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import library for scikit plot and ks plot.\n",
        "import scikitplot as skplt"
      ],
      "metadata": {
        "id": "DCZpQW6NSTfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the function to visualize the model preformence.\n",
        "def model_vis(y_train,y_test,y_pred_train,y_pred_test,y_train_proba,y_test_proba):\n",
        "  '''\n",
        "  This function helps to visualize the confusion matrix of train set and test set and ks chart and roc curve\n",
        "  '''\n",
        "  fig,ax=plt.subplots(2,2,figsize=(15,10))\n",
        "  cm = metrics.confusion_matrix(y_train,y_pred_train)\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix= cm, display_labels=['Non Defaulter', 'Defaulter'])\n",
        "  cm_display.plot(ax=ax[0,0])\n",
        "  ax[0,0].set_title('Confusion matrix of Training set prediction ')\n",
        "  # Confusion metric for test.\n",
        "  cm = confusion_matrix(y_test, y_pred_test)\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix= cm, display_labels=['Non Defaulter', 'Defaulter'])\n",
        "  cm_display.plot(ax=ax[0,1])\n",
        "  ax[0,1].set_title('Confusion matrix of Test set Prediction')\n",
        "\n",
        "  # ks chart plot\n",
        "  skplt.metrics.plot_ks_statistic(y_train, y_train_proba, ax=ax[1,0])\n",
        "  # Remove the last subplot\n",
        "  skplt.metrics.plot_ks_statistic(y_test, y_test_proba, ax=ax[1,1])\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "U-Xt00YLSkoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model preformence.\n",
        "model_vis(y_train,y_test,y_pred_train,y_pred_test,lr_train_proba,lr_test_proba)"
      ],
      "metadata": {
        "id": "bYouNhKEXzzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* There is similar outcome of training set and test set.\n",
        "* According to the test set,around 90% of all defaulters are correctly anticipated, and approximately 70% of all defaulters are successfully predicted.\n",
        "* 0.655 is going to be our threshould. We being the solution of the business case by considering as potential defaulter all the observations with a predicted probability of defaulting greater than 0.655\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dy8xv3PnY2fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a empty datafram with column name of metric chart.\n",
        "metrics_df = pd.DataFrame(columns=['Model','Accuracy','Precision','Recall','F1 Score','KS_Stat'])\n",
        "def metric_score(model_name, y_test, y_pred_test, y_proba):\n",
        "  '''\n",
        "  This function will determine model evalation metrics with a dataframe.\n",
        "  '''\n",
        "  # accuracy.\n",
        "  acc= accuracy_score(y_test, y_pred_test)\n",
        "  # precision.\n",
        "  prec = precision_score(y_test, y_pred_test)\n",
        "  # Recall.\n",
        "  rec = recall_score(y_test, y_pred_test)\n",
        "  # F1 score.\n",
        "  f1 = f1_score(y_test, y_pred_test)\n",
        "  # Creating a dataframe for ks stat calculation.\n",
        "  df = pd.DataFrame()\n",
        "  df['real'] = y_test\n",
        "  df['proba'] = y_proba[:, 1]\n",
        "\n",
        "  # Recovering each class.\n",
        "  class0 = df[df['real'] == 0]\n",
        "  class1 = df[df['real'] == 1]\n",
        "\n",
        "  ks = ks_2samp(class0['proba'], class1['proba'])\n",
        "  stat = round(ks[0],3)\n",
        "\n",
        "  # dataframe of all metric score.\n",
        "  eval_metrics_list = [model_name, acc, prec, rec, f1, stat]\n",
        "  metrics_df.loc[len(metrics_df)] = eval_metrics_list\n",
        "  return metrics_df"
      ],
      "metadata": {
        "id": "N9Wi9-I0_lJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view all the metrics score in a dataframe.\n",
        "metric_score('Logistic_Regression', y_test, y_pred_test, lr_test_proba)"
      ],
      "metadata": {
        "id": "uwDj_75WEP2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "lr = LogisticRegression()\n",
        "param_grid = {'penalty':['l2', 'l1'], 'C':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5]}\n",
        "lr_cv = GridSearchCV(lr, param_grid, scoring = 'recall', n_jobs = -1, verbose =3, cv = 5)\n",
        "# Fit the Algorithm\n",
        "lr_cv.fit(X_train_scaled,y_train)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Parameters.\n",
        "print('Best Parameters', lr_cv.best_params_)\n",
        "lr_best = lr_cv.best_estimator_\n",
        "\n",
        "# Predicting the probabilities of test data.\n",
        "lr_best_test_proba = lr_best.predict_proba(X_test_scaled)\n",
        "lr_best_train_proba = lr_best.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y from x via model.\n",
        "y_best_pred_test = lr_best.predict(X_test_scaled)\n",
        "y_best_pred_train = lr_best.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "4iR8U_F3NhND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model performence.\n",
        "model_vis(y_train,y_test,y_best_pred_train,y_best_pred_test,lr_best_train_proba,lr_best_test_proba)"
      ],
      "metadata": {
        "id": "WKwpOyWmPtbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in a dataframe.\n",
        "metric_score('Tuned Logistic_Regression',y_test, y_best_pred_test, lr_best_test_proba)"
      ],
      "metadata": {
        "id": "baSP3yjuQxzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSerch CV to discover the best parameters for the model in order to improve its accuracy. GridSerchCV is the process of tweaking hyperparameters to determine the best value for a particular model. The value of hyperparameter has a substantial impact on model performance.\n",
        "\n",
        "GridSerchCV examines the model for each combination of the value supplied in the dictionary using the cross validation technique.As a result of utilising this function , we can calculate the accuracy/loss for each combination of hyperparameter and select the one with the best performance."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe from the above that, with the exception of the ks statictics value, all evaluation parameters have changed.Only precision has been significantly reduced, while the rest have been slightly increased because the model dependent data is unbalanced and our target population is small in contrast to the non-target population, we prioritised recall value improvement. we can say that model performance has not improved significantly because the recall value has increased slightly after adjustment."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2   Decision Tree classifier."
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML model - implementation .\n",
        "model_df = DecisionTreeClassifier(criterion =\"gini\", random_state=100, min_samples_leaf= 10, max_depth=10)\n",
        "\n",
        "# Fit the Algorithm\n",
        "model_df.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "NI0p6nvJBjF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the probabilities of test data.\n",
        "dt_test_proba = model_df.predict_proba(X_test_scaled)\n",
        "dt_train_proba = model_df.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y for x via model.\n",
        "y_pred_test = model_df.predict(X_test_scaled)\n",
        "y_pred_train = model_df.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "VNbzqab1C507"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart.\n",
        "model_vis(y_train,y_test,y_pred_train,y_pred_test,dt_train_proba,dt_test_proba)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in a dataframe.\n",
        "metric_score('Decision_Tree Classifier', y_test,y_pred_test,dt_test_proba)"
      ],
      "metadata": {
        "id": "PdYud41eFER8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "dt  = DecisionTreeClassifier()\n",
        "param_grid = param_dict = {'max_depth': [20,25,30,35],\n",
        "                           'min_samples_split' : [50,100,150],\n",
        "                           'min_samples_leaf' : [40,50,60]}\n",
        "dt_cv = GridSearchCV(dt,param_grid, scoring = 'recall', n_jobs = -1, verbose = 3, cv = 5)\n",
        "# Fit the Algorithm\n",
        "dt_cv.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Parameters.\n",
        "print('Best Parameters :', dt_cv.best_params_)\n",
        "dt_best = dt_cv.best_estimator_\n",
        "\n",
        "# Prediciting the probabilities of test data.\n",
        "dt_best_test_proba = dt_best.predict_proba(X_test_scaled)\n",
        "dt_best_train_proba = dt_best.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y from x via model.\n",
        "y_best_pred_test = dt_best.predict(X_test_scaled)\n",
        "y_best_pred_train = dt_best.predict(X_train_scaled)\n"
      ],
      "metadata": {
        "id": "lJpaZx0iIROa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the model performence.\n",
        "model_vis(y_train,y_test,y_best_pred_train,y_best_pred_test,dt_best_train_proba,dt_test_proba)"
      ],
      "metadata": {
        "id": "Dhx7aYVOKm0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in a dataframe.\n",
        "metric_score('Tuned Decision_Tree_Classifier', y_test, y_best_pred_test, dt_best_test_proba)"
      ],
      "metadata": {
        "id": "AOqK9WOwLdiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Used GrideSearchCV to discover the best parameters for the model in order to improve its accuracy. GrideSearchCV is process of tweaking hyperparameters to determine the best value for particular model. The value of hyperparameter has a substantial impact on model performance.\n",
        "\n",
        "GrideSearchCV examines the model for each combination of the value supplied in the dictionary using the cross validation technique. As a result of ultilising this function, we can calculate the accuracy/loss for each combination of hyperparameters and select the one with the best performence."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe from the above that, with the exception of the ks statictis value all evaluation parameters have changed. Only precision has been significantly reduced, while the rest have been slightly increased. We can say that model performance has not improved significantly because the recall value has slightly increased after adjustment."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using Recall primarily for model evaluation beacuse False Negative indicates that a person will not default when they actually do and acknowledging defaulter clinent as non defaulter, will result in large loss for the bank, thus we must reduce the False Negative, and as False Negative decreases, Recall will grow."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3    Random Forest."
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "model_rf = RandomForestClassifier(n_estimators= 100, criterion= 'entropy', random_state= 0)\n",
        "# Fit the Algorithm\n",
        "model_rf.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediciting the probabilities of test data.\n",
        "rf_test_proba = model_rf.predict_proba(X_test_scaled)\n",
        "rf_train_proba = model_rf.predict_proba(X_train_scaled)\n",
        "# Predicting the value of y from x via model.\n",
        "y_pred_test = model_rf.predict(X_test_scaled)\n",
        "y_pred_train = model_rf.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "Dko1QL5oNZrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_vis(y_train, y_test, y_pred_train, y_pred_test, rf_train_proba, rf_test_proba)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in a dataframe.\n",
        "metric_score('RandomForest_Classifier', y_test, y_pred_test, rf_test_proba)"
      ],
      "metadata": {
        "id": "z1QvjT9FRHQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "rf = RandomForestClassifier()\n",
        "param_grid = {'n_estimators' : [100,250],\n",
        "              'max_depth' : [None, 80, 100],\n",
        "              'min_samples_split' : [10, 2],\n",
        "              'min_samples_leaf' : [1, 10]}\n",
        "rf_cv = GridSearchCV(rf, param_grid, scoring ='recall', n_jobs= -1, verbose= 3, cv = 5)\n",
        "# Fit the Algorithm\n",
        "rf_cv.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Parameters.\n",
        "print('Best parameters :', rf_cv.best_params_)\n",
        "rf_best = rf_cv.best_estimator_\n",
        "\n",
        "# Predicting the probabilities of test data.\n",
        "rf_best_test_proba = rf_best.predict_proba(X_test_scaled)\n",
        "rf_best_train_proba = rf_best.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y from x via model.\n",
        "y_best_pred_test = rf_best.predict(X_test_scaled)\n",
        "y_best_pred_train = rf_best.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "Dvu46941Y7wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_rep = classification_report(y_test, y_best_pred_test)\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "mr-V3H0mcPGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model preformence.\n",
        "model_vis(y_train,y_test,y_best_pred_train,y_best_pred_test,rf_best_train_proba,rf_best_test_proba)"
      ],
      "metadata": {
        "id": "Y1VgmLnAc2pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in dataframe.\n",
        "metric_score('Tuned Random_Forest_Classifier',y_test,y_best_pred_test,rf_best_test_proba)"
      ],
      "metadata": {
        "id": "jZD3mEsndrXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Used GrideSearchCV to discover the best parameters for the model in order to improve its accuracy. GrideSearchCV is process of tweaking hyperparameters to determine the best value for particular model. The value of hyperparameter has a substantial impact on model performance.\n",
        "\n",
        "GrideSearchCV examines the model for each combination of the value supplied in the dictionary using the cross validation technique. As a result of ultilising this function, we can calculate the accuracy/loss for each combination of hyperparameters and select the one with the best performence."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance of model doesn't improved after cross validation so we can say that our base model was performing good with Recall of 84%"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ML Model-4    K-Nearest Neighbors(KNN)"
      ],
      "metadata": {
        "id": "eKXWvOEggGEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "model_knn = KNeighborsClassifier(n_neighbors=7)\n",
        "# Fit the Algorithm\n",
        "model_knn.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "Spwo1dQ4gfkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the probabilities of test data.\n",
        "knn_test_proba = model_knn.predict_proba(X_test_scaled)\n",
        "knn_train_proba = model_knn.predict_proba(X_train_scaled)\n",
        "# Predicting the value of y from x via model.\n",
        "y_pred_test = model_knn.predict(X_test_scaled)\n",
        "y_pred_train = model_knn.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "CdTH9Y0EhAVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "bPeTuCUOiM8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-nearest neighbour algorithm, often known as KNN or K-NN, is a non paramatric, supervised learning classifier that employs proximity to classify or predict the grouping of single data point. while it can be used for either regression or classification issues. it is the most commonly utilised as a classification algorithm, based on the idea that similar point can be discovered nearby."
      ],
      "metadata": {
        "id": "_h0mEuziiOQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the performance of model.\n",
        "model_vis(y_train,y_test,y_pred_train,y_pred_test,knn_train_proba,knn_test_proba)"
      ],
      "metadata": {
        "id": "14sLHxqSjfQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  View all the metric score in a dataframe.\n",
        "metric_score('KNN_Classifier',y_test,y_pred_test,knn_test_proba)"
      ],
      "metadata": {
        "id": "MXiTq-D2kARg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "qtmzhqeSkdma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "knn = KNeighborsClassifier()\n",
        "param_grid = {'n_neighbors' : [3,5,7,9,11],\n",
        "              'weights' : ['uniform', 'distance'],\n",
        "              'p' : [1, 2]}\n",
        "knn_cv = GridSearchCV(knn, param_grid, scoring= 'recall', n_jobs = -1 , verbose = 3, cv = 5)\n",
        "# Fit the algorithm.\n",
        "knn_cv.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "Gt_-mGKVkoUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters.\n",
        "print('Best Parameters', knn_cv.best_params_)\n",
        "knn_best = knn_cv.best_estimator_\n",
        "\n",
        "# Prediciting the probabilities of test data.\n",
        "knn_best_test_proba = knn_best.predict_proba(X_test_scaled)\n",
        "knn_best_train_proba = knn_best.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y from x via model.\n",
        "y_best_pred_test = knn_best.predict(X_test_scaled)\n",
        "y_best_pred_train = knn_best.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "TQON_RGG1vG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model performence.\n",
        "model_vis(y_train, y_test, y_best_pred_train, y_best_pred_test,knn_best_train_proba, knn_best_test_proba)"
      ],
      "metadata": {
        "id": "dbKK9BkJ4C7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in dataframe.\n",
        "metric_score('Tuned KNN_Classifier', y_test, y_best_pred_test, knn_best_test_proba)"
      ],
      "metadata": {
        "id": "I50Yx9jj4uC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "SncYfUmx5SvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Used GrideSearchCV to discover the best parameters for the model in order to improve its accuracy. GrideSearchCV is process of tweaking hyperparameters to determine the best value for particular model. The value of hyperparameter has a substantial impact on model performance.\n",
        "\n",
        "GrideSearchCV examines the model for each combination of the value supplied in the dictionary using the cross validation technique. As a result of ultilising this function, we can calculate the accuracy/loss for each combination of hyperparameters and select the one with the best performence."
      ],
      "metadata": {
        "id": "_f8Paftg5bm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "bysU5WLK5lWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Performance of our model has been imporved with improvment in all evaluation metrics. After cross validatoin recall become around 85%"
      ],
      "metadata": {
        "id": "_Alczjjh5mbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ML Model-5   AdaBoost Classifier"
      ],
      "metadata": {
        "id": "j6RNi4NA6QLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation\n",
        "model_ada = AdaBoostClassifier(n_estimators=100)\n",
        "# Fit the Algorithm.\n",
        "model_ada.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "14V7EcW66jTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the probabulities of test data.\n",
        "ada_test_proba = model_ada.predict_proba(X_test_scaled)\n",
        "ada_train_proba = model_ada.predict_proba(X_train_scaled)\n",
        "# Predicitng the value of y from x via model.\n",
        "y_pred_test = model_ada.predict(X_test_scaled)\n",
        "y_pred_train = model_ada.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "z4bvRtG47KJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "hB9N1pNn745Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model performance.\n",
        "model_vis(y_train, y_test, y_pred_train, y_pred_test, ada_train_proba, ada_test_proba)"
      ],
      "metadata": {
        "id": "o6ekkQKC8CSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all metric score in dataframe.\n",
        "metric_score('AdaBoost Classifier',y_test,y_pred_test,ada_test_proba)"
      ],
      "metadata": {
        "id": "7gYAL4SZdvua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "s7cORJQh--Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "ada = AdaBoostClassifier()\n",
        "param_grid = {'n_estimators' : [50,90,120,180,200],\n",
        "              'learning_rate' : [0.001,0.01,0.1,1,10]}\n",
        "ada_cv = GridSearchCV(ada, param_grid, scoring = 'recall', n_jobs = -1, verbose= 3, cv =5)\n",
        "# Fit the algorithm.\n",
        "ada_cv.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "8Fh3DPLp_FNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Parameters.\n",
        "print('Best Parameters:',ada_cv.best_params_)\n",
        "ada_best = ada_cv.best_estimator_\n",
        "\n",
        "# Prediciting the probabilites of test data.\n",
        "ada_best_test_proba = ada_best.predict_proba(X_test_scaled)\n",
        "ada_best_train_proba = ada_best.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y from x via model.\n",
        "y_best_pred_test = ada_best.predict(X_test_scaled)\n",
        "y_best_pred_train = ada_best.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "znWki55GeiHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model performence.\n",
        "model_vis(y_train,y_test,y_best_pred_train,y_best_pred_test,ada_best_train_proba,ada_best_test_proba)"
      ],
      "metadata": {
        "id": "SE9YZuFbishn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all metric score in dataframe.\n",
        "metric_score('Tune AdaBoost_Classifier',y_test,y_best_pred_test,ada_best_test_proba)"
      ],
      "metadata": {
        "id": "2wKY8_Uhjnyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because GridsearchCV was taking too long, we utilised RandomizedSearchCV, which solves the problem of GridsearchCV by going through only a predetermined number of hyperparameter values. it moves randomly throughout the grid to discove the best collection of hyperparameters. This method eliminates unnecessary computation. it uses the value distribution."
      ],
      "metadata": {
        "id": "kBjDsIetkhdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "tsZB5vGLmVEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is little improvement in performence of our model, recall value increase 0.76 from  0.75."
      ],
      "metadata": {
        "id": "Ytup5VYEmWLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ML Model-6  **XGboost**"
      ],
      "metadata": {
        "id": "cOmvGk6fmjgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost.sklearn import XGBRFClassifier\n",
        "#Implementing Model-6\n",
        "model_xg = XGBClassifier()\n",
        "# Fit the algorithem.\n",
        "model_xg.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "zVggyQUqmydA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the probabilities of test data.\n",
        "xg_test_proba = model_xg.predict_proba(X_test_scaled)\n",
        "xg_train_proba = model_xg.predict_proba(X_train_scaled)\n",
        "\n",
        "# Predicting the value of y from x via model.\n",
        "y_pred_test = model_xg.predict(X_test_scaled)\n",
        "y_pred_train = model_xg.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "M4yDB40Bnja3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "EwBuHJcWoiG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model performence.\n",
        "model_vis(y_train,y_test,y_pred_train,y_pred_test,xg_train_proba,xg_test_proba)"
      ],
      "metadata": {
        "id": "u3pLo294olck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric score in dataframe.\n",
        "metric_score('XGB_Classifier',y_test,y_pred_test,xg_test_proba)"
      ],
      "metadata": {
        "id": "bbUL490ZpN7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-BpETf-cprPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "xg = XGBClassifier()\n",
        "param_grid = {'n_estimators': [50,100,150], 'max_depth' : [3,5,9]}\n",
        "xg_cv = GridSearchCV(xg, param_grid, scoring = 'recall', n_jobs = -1, verbose= 3, cv = 5)\n",
        "# Fit the algorithm.\n",
        "xg_cv.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "YrCWDtkUpuqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Parameters.\n",
        "print('Best Parameters:', xg_cv.best_params_)\n",
        "xg_best = xg_cv.best_estimator_\n",
        "\n",
        "# Predicting the probabilities of test data.\n",
        "xg_best_test_proba = xg_best.predict_proba(X_test_scaled)\n",
        "xg_best_train_proba = xg_best.predict_proba(X_train_scaled)\n",
        "\n",
        "# Preddicting the value of y from x via model.\n",
        "y_best_pred_test = xg_best.predict(X_test_scaled)\n",
        "y_best_pred_train = xg_best.predict(X_train_scaled)"
      ],
      "metadata": {
        "id": "ZxKgnioSrIvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model performence.\n",
        "model_vis(y_train,y_test,y_best_pred_train,y_best_pred_test,xg_best_train_proba,xg_best_test_proba)"
      ],
      "metadata": {
        "id": "1KI-ORnyscbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View all the metric socre in dataframe.\n",
        "metric_score('Tuned XGB_Classifier', y_test,y_best_pred_test,xg_best_test_proba)"
      ],
      "metadata": {
        "id": "DaNISf2rs9jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using Recall Primarily for model evaluation because False Negative indicate that a person will not default when they actually do, and recognising defaulter clients and non-defaulter will result in a large loss for the bank, thus we must reduce False Negative, and as False Negative decreases, Recall will grow."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Will use Random Forest Classifier as final prediction model because its recall is approximately 84.32% which is greater than other models in both case.\n",
        "\n",
        "KNN is not a memory-efficient algorithm. As a model need to store all the data point, it gets exceedingly sluggish as the number of data points increases. it is computationally intensive since the algorithm must calculate the distance between all the data points and determine the nearest neighbours for each datapoint."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Model explaination we will use LIME method.\n",
        "\n",
        "LIME stands for Local Interpretable Model-agnostic Explanation.LIME focuses on training local surrogate models to explain individual prediction. Local surrogate model are interpretable models that are used to explain individual predicition of black box machine learning models. Surrogate model are training to approximate the predictions of the underlying black box model. Instaed of training a globle surrogate model, LIME focuses on training local surrogate models."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install lime"
      ],
      "metadata": {
        "id": "y1-AvsL2st_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing lime library.\n",
        "import lime\n",
        "import lime.lime_tabular"
      ],
      "metadata": {
        "id": "qeMW2ivHs5PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lime had one explainer for all models.\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names= X_train.columns.values.tolist(),\n",
        "                                                   class_names=['Non-Defaulter', 'Defaulter'], verbose= True, mode= 'classification')"
      ],
      "metadata": {
        "id": "vSwVpFV6tLdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the 5th instance and use it to predict the results.\n",
        "j = 5\n",
        "exp = explainer.explain_instance(X_test.values[j], rf_best.predict_proba, num_features= X_train_scaled.shape[1])"
      ],
      "metadata": {
        "id": "kH0y09e2uMb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the predictions.\n",
        "exp.show_in_notebook(show_table= True)"
      ],
      "metadata": {
        "id": "lHzzxdVXvIjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* We saw that Our dataset had an uneven class.\n",
        "* The majority of credit card holders were female, while male clients had a high default ration.\n",
        "* Higher educated people are less likely to default, whereas fewer educated people are more likely to default.\n",
        "* The likelihood of default increases after the age of 60.\n",
        "* In this scenario, 'Recall' is a significant parameter to compare all of the algorithm.Because the corporation cannot afford to anticipate a false negative, that is to predict a defaulter as a non-defaulter. Since the corporation is the one who would deliver money to consumers, if for any reason, sending money to defaulter increases the risk of obtaining the investment returned, As a result, recognising false negatives is critical in this case.    \n",
        "* The Random forest Classifier model has a high recall(84.32%).\n",
        "* We will not use KNN as our basic model because it has a similar recall to Random Forest Classifier but its bestparams n_neighbors is 1 and can be noisy, leading to outlier effects in the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}